{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74666b6b",
   "metadata": {},
   "source": [
    "# NEWEST "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fba4d7-2e0c-4df8-b3a6-1902c513bba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IW CODE: https://github.com/sarahmbock/COS-IW-BOCK/blob/main/DANN%20CODE.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee7fc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import torch.utils.data as utils\n",
    "import time\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Function\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import numpy as np \n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856725d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient reversal layer\n",
    "class GRL(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, constant):\n",
    "        ctx.constant = constant\n",
    "        return x.view_as(x) * constant\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return grad_output.neg() * ctx.constant, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f204122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "\n",
    "class GradReverse(Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_):\n",
    "        ctx.lambda_ = lambda_\n",
    "        return x.view_as(x)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.lambda_\n",
    "        return output, None\n",
    "\n",
    "def grad_reverse(x, lambda_=1.0):\n",
    "    return GradReverse.apply(x, lambda_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34bedb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dann(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Dann, self).__init__()\n",
    "        self.f = nn.Sequential(\n",
    "                nn.Linear(5112, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        # Label classifier\n",
    "        self.lc = nn.Sequential(\n",
    "    nn.Linear(256, 256),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(256, 56),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(56, 2),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "        \n",
    "        # Domain classifier\n",
    "        self.dc = nn.Sequential(nn.Linear(256, 128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128, 128), nn.Sigmoid(),              \n",
    "    nn.Linear(128, 2),\n",
    "    nn.Sigmoid()\n",
    "                                \n",
    ")\n",
    "    def forward(self, x,alpha):\n",
    "        x = self.f(x)\n",
    "        x = x.view(-1, 256)\n",
    "        y = GRL.apply(x, alpha)\n",
    "        x = self.lc(x)\n",
    "        y = self.dc(y)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        y=y.view(y.shape[0],-1)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80048c5c",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5588f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import rowgenerators as rg ****commented out for thesis****\n",
    "\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "scfp = pd.read_stata('p19i6.dta')\n",
    "\n",
    "\n",
    "\n",
    "# SELECT only rows where there is debt and where white and black\n",
    "delinquency = 'x3005'\n",
    "loan_schedule ='x3004'# 1 on time, 5 sometimes late, 0 inapp.\n",
    "race = 'x6809'\n",
    "additional_race = 'x6810'\n",
    "hispanic = 'x7004'\n",
    "\n",
    "scfp = scfp[(scfp[race] == 1) | (scfp[race] == 2)]\n",
    "scfp =  scfp[(scfp[loan_schedule] == 1) | (scfp[loan_schedule] == 5)]\n",
    "scfp.shape\n",
    "\n",
    "# Remove revealing columns\n",
    "race_revealing = [additional_race, 'x7004']\n",
    "race_revealing += [col.replace('x', 'j') for col in race_revealing]\n",
    "sched_revealing = ['x7571', 'x7570', 'x7569', 'x7556', 'x7564', 'x7554','x7553',\n",
    "                   'x7534', 'x7533', 'x7532', 'x7166', 'x7529', 'x7821', 'x7844',\n",
    "                   'x7867', 'x7921', 'x7569', 'x7944', 'x7521', 'x7520', 'x7518',\n",
    "                   'x7517', delinquency, 'j3005', 'j3004']\n",
    "sched_revealing += [col.replace('x', 'j') for col in sched_revealing]\n",
    "\n",
    "exclude = sched_revealing+ race_revealing\n",
    "X = scfp.copy()\n",
    "# Eliminate bad columns\n",
    "X = scfp.loc[:, [col for col in scfp.columns if col not in exclude]]\n",
    "X = X.fillna(-1)\n",
    "\n",
    "# Remove columns with all -1\n",
    "\n",
    "# instantiate the VarianceThreshold object with a threshold of 0\n",
    "selector = VarianceThreshold(threshold=0)\n",
    "\n",
    "# fit the selector on the DataFrame\n",
    "selector.fit(X)\n",
    "\n",
    "# get the indices of the non-constant columns\n",
    "non_constant_cols = selector.get_support(indices=True)\n",
    "\n",
    "# select the non-constant columns from the original DataFrame\n",
    "X = X.iloc[:, non_constant_cols]\n",
    "\n",
    "# print the filtered DataFrame\n",
    "print(X.shape)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# split to source (white) and target (black)\n",
    "# First, create a new column in X that indicates whether each person is white or black\n",
    "print(len(X))\n",
    "# Use groupby to split X and y into two separate dataframes, one with only white data and the other with only black data\n",
    "X_white = X[X[race] == 1]\n",
    "y_white = X_white[loan_schedule].replace({1: 0, 5: 1})\n",
    "X_white = X_white.drop([race, loan_schedule], axis =1)\n",
    "#print(y_white)\n",
    "print(len(y_white))\n",
    "print(len(X_white))\n",
    "\n",
    "X_black = X[X[race]==2]\n",
    "y_black = X_black[loan_schedule].replace({1: 0, 5: 1})\n",
    "X_black = X_black.drop([race, loan_schedule], axis =1)\n",
    "#print(y_black)\n",
    "print(len(y_black))\n",
    "print(len(X_black))\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_tensor_source =  torch.from_numpy(X_white.values).type(torch.float32)\n",
    "X_tensor_target =  torch.from_numpy(X_black.values).type(torch.float32)\n",
    "y_tensor_source =torch.tensor(y_white.values).type(torch.LongTensor)\n",
    "y_tensor_target =torch.tensor(y_black.values).type(torch.LongTensor)\n",
    "\n",
    "\n",
    "# Split the source (race) dataset into training and test sets\n",
    "X_train_source, X_test_source, y_train_source, y_test_source = train_test_split(X_tensor_source, y_tensor_source, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Split the target (loan sched) into training and test sets\n",
    "X_train_target, X_test_target, y_train_target, y_test_target = train_test_split(X_tensor_target, y_tensor_target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDatasets from the training and test sets\n",
    "target_train = TensorDataset(X_train_target, y_train_target)\n",
    "target_test = TensorDataset(X_test_target, y_test_target)\n",
    "\n",
    "source_train = TensorDataset(X_train_source, y_train_source)\n",
    "source_test = TensorDataset(X_test_source, y_test_source)\n",
    "\n",
    "# WEIGHTS\n",
    "num_samples = len(target_train)\n",
    "num_positives = sum(target_train[i][1] for i in range(num_samples))\n",
    "num_negatives = num_samples - num_positives\n",
    "class_sample_count = np.array([num_negatives, num_positives])\n",
    "\n",
    "weight = 1/class_sample_count\n",
    "print('positive weight (under represented):', weight[1])\n",
    "print('negative weight (over represented):', weight[0])\n",
    "\n",
    "samples_weight = np.array([weight[t] for t in y_train_target])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Create DataLoaders from the datasets\n",
    "batch_size = 64\n",
    "target_train = DataLoader(target_train, batch_size=batch_size, sampler=sampler)\n",
    "target_test= DataLoader(target_test, batch_size=len(target_test), shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "num_samples = len(source_train)\n",
    "num_positives = sum(source_train[i][1] for i in range(num_samples))\n",
    "num_negatives = num_samples - num_positives\n",
    "class_sample_count = np.array([num_negatives, num_positives])\n",
    "\n",
    "weight = 1/class_sample_count\n",
    "print('positive weight (under represented):', weight[1])\n",
    "print('negative weight (over represented):', weight[0])\n",
    "\n",
    "samples_weight = np.array([weight[t] for t in y_train_source])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "\n",
    "source_train = DataLoader(source_train, batch_size=batch_size, sampler = sampler)\n",
    "source_test = DataLoader(source_test, batch_size=len(source_test), shuffle=False)\n",
    "\n",
    "print(X_train_source.shape)\n",
    "print(X_train_target.shape)\n",
    "combined_tensor = torch.cat((X_train_source, X_train_target), dim=0)\n",
    "\n",
    "\n",
    "# ****** what am i weighting based off of\n",
    "y_domain = torch.zeros(len(combined_tensor), dtype=torch.long)\n",
    "y_domain[len(X_train_source):] = 1\n",
    "print(sum(y_domain).item())\n",
    "print(y_domain)\n",
    "combined_set =  TensorDataset(combined_tensor, y_domain)\n",
    "class_sample_count = np.array([len(X_train_source), len(X_train_target)])\n",
    "print(class_sample_count)\n",
    "weight = 1/class_sample_count\n",
    "print(weight)\n",
    "samples_weight = np.array([weight[t] for t in y_domain])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "\n",
    "combined_loader =  DataLoader(combined_set, batch_size=batch_size, sampler=sampler)\n",
    "\n",
    "\n",
    "\n",
    "# Combine white and black to train credit predictor\n",
    "y_label_combined = torch.cat((y_train_source,y_train_target), dim=0)\n",
    "X_label_combined = torch.cat((X_train_source,X_train_target), dim=0)\n",
    "num_samples = len(X_label_combined)\n",
    "num_positives = sum(y_label_combined[i] for i in range(num_samples))\n",
    "num_negatives = num_samples - num_positives\n",
    "print(num_positives.item())\n",
    "print(num_negatives.item())\n",
    "class_sample_count = np.array([num_negatives.item(), num_positives.item()])\n",
    "weight = 1/class_sample_count\n",
    "print(weight)\n",
    "samples_weight = np.array([weight[t] for t in y_label_combined])\n",
    "samples_weight = torch.from_numpy(samples_weight)\n",
    "sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n",
    "combined_label_set = TensorDataset(X_label_combined, y_label_combined)\n",
    "combined_label_loader = DataLoader(combined_label_set, batch_size=batch_size, sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd3f704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, source_train_loader, source_test_loader, target_test_loader, lambda_):\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "    model.lc.eval()\n",
    "    model.dc.eval()\n",
    "    model.f.eval()\n",
    "\n",
    "    # Define variables for tracking accuracy\n",
    "    source_label_correct = 0\n",
    "    source_domain_correct = 0\n",
    "    target_domain_correct = 0\n",
    "\n",
    "    # Evaluate on source train set\n",
    "    for i, (data, labels) in enumerate(source_train_loader):\n",
    "        # Set domain label to 0 (i.e., source domain)\n",
    "        domain_labels = torch.zeros(data.shape[0]).long()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        label_preds, domain_preds = model(data, alpha=lambda_)\n",
    "        #print(domain_preds)\n",
    "        domain_preds = model.dc(grad_reverse(model.f(data), lambda_))\n",
    "        \n",
    "\n",
    "        # Calculate accuracy of label classifier\n",
    "        _, label_preds = torch.max(label_preds, 1)\n",
    "        source_label_correct += (label_preds == labels).sum().item()\n",
    "\n",
    "        # Calculate accuracy of domain classifier\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        source_domain_correct += (domain_preds == domain_labels).sum().item()\n",
    "    #num_ones_predicted =0\n",
    "    # Evaluate on source test set\n",
    "    for i, (data, labels) in enumerate(source_test_loader):\n",
    "        # Set domain label to 0 (i.e., source domain)\n",
    "        domain_labels = torch.zeros(data.shape[0]).long()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        label_preds, domain_preds = model(data, alpha=lambda_)\n",
    "        domain_preds = model.dc(grad_reverse(model.f(data), lambda_))\n",
    "\n",
    "        # Calculate accuracy of label classifier\n",
    "        _, label_preds = torch.max(label_preds, 1)\n",
    "        source_label_correct += (label_preds == labels).sum().item()\n",
    "        #num_ones_predicted += list(label_preds.numpy()).count(1)\n",
    "\n",
    "        # Calculate accuracy of domain classifier\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        source_domain_correct += (domain_preds == domain_labels).sum().item()\n",
    "    \n",
    "    # Evaluate on target test set\n",
    "    target_label_correct =0\n",
    "    for i, (data, labels) in enumerate(target_test_loader):\n",
    "        # Set domain label to 1 (i.e., target domain)\n",
    "        domain_labels = torch.ones(data.shape[0]).long()\n",
    "\n",
    "        # Forward pass through the model\n",
    "        label_preds, domain_preds = model(data, alpha=lambda_)\n",
    "        domain_preds = model.dc(grad_reverse(model.f(data), lambda_))\n",
    "        \n",
    "        # Calculate accuracy of label classifier\n",
    "        _, label_preds = torch.max(label_preds, 1)\n",
    "        target_label_correct += (label_preds == labels).sum().item()\n",
    "        #num_ones_predicted += list(label_preds.numpy()).count(1)\n",
    "        \n",
    "        # Calculate accuracy of domain classifier\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        target_domain_correct += (domain_preds == domain_labels).sum().item()\n",
    "        \n",
    "\n",
    "    # Calculate and print accuracy\n",
    "    source_label_accuracy = 100. * source_label_correct / (len(source_train_loader.dataset) + len(source_test_loader.dataset))\n",
    "    source_domain_accuracy = 100. * source_domain_correct / (len(source_train_loader.dataset) + len(source_test_loader.dataset))\n",
    "    target_label_accuracy = 100. * target_label_correct / (len(target_test_loader.dataset))\n",
    "    target_domain_accuracy = 100. * target_domain_correct / len(target_test_loader.dataset)\n",
    "    print('Source Label Accuracy: {:.2f}%'.format(source_label_accuracy))\n",
    "    print('Target Label Accuracy: {:.2f}%'.format(target_label_accuracy))\n",
    "    print('Source Domain Accuracy: {:.2f}%'.format(source_domain_accuracy))\n",
    "    print('Target Domain Accuracy: {:.2f}%'.format(target_domain_accuracy))\n",
    "    #print('Number of loan defaults predicted: ', num_ones_predicted / (len(target_test_loader.dataset)+ len(source_test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabe5ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "def test_fairness(model, lambda_):\n",
    "    with torch.no_grad():\n",
    "        predicted_labels_white = []\n",
    "        true_labels_white = []\n",
    "        predicted_domain = []\n",
    "        source_label_correct =0\n",
    "        source_domain_correct =0\n",
    "        for i, (data, labels) in enumerate(source_test):\n",
    "            # Set domain label to 0 (i.e., source domain)\n",
    "            domain_labels = torch.zeros(data.shape[0]).long()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            label_preds, domain_preds = model(data, alpha=lambda_)\n",
    "            domain_preds = model.dc(grad_reverse(model.f(data), lambda_=lambda_))\n",
    "\n",
    "            # Calculate accuracy of label classifier\n",
    "            _, label_preds = torch.max(label_preds, 1)\n",
    "            source_label_correct += (label_preds == labels).sum().item()\n",
    "            #num_ones_predicted += list(label_preds.numpy()).count(1)\n",
    "\n",
    "            # Calculate accuracy of domain classifier\n",
    "            _, domain_preds = torch.max(domain_preds, 1)\n",
    "            source_domain_correct += (domain_preds == domain_labels).sum().item()\n",
    "            predicted_labels_white += label_preds\n",
    "            true_labels_white += labels\n",
    "            predicted_domain += domain_preds\n",
    "        conf_matrix = confusion_matrix(true_labels_white, predicted_labels_white)\n",
    "        print('Credit Confusion Matrix White: ')\n",
    "        print(conf_matrix)\n",
    "        fpr = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0]) \n",
    "        print('False positive white: ', fpr)\n",
    "        \n",
    "        predicted_labels = []\n",
    "        true_labels = []\n",
    "        predicted_domain = []\n",
    "        target_label_correct =0\n",
    "        target_domain_correct =0\n",
    "        for i, (data, labels) in enumerate(target_test):\n",
    "            # Set domain label to 0 (i.e., source domain)\n",
    "            domain_labels = torch.ones(data.shape[0]).long()\n",
    "\n",
    "            # Forward pass through the model\n",
    "            label_preds, domain_preds = model(data, alpha=lambda_)\n",
    "            domain_preds = model.dc(grad_reverse(model.f(data), lambda_=lambda_))\n",
    "\n",
    "            # Calculate accuracy of label classifier\n",
    "            _, label_preds = torch.max(label_preds, 1)\n",
    "            target_label_correct += (label_preds == labels).sum().item()\n",
    "            #num_ones_predicted += list(label_preds.numpy()).count(1)\n",
    "\n",
    "            # Calculate accuracy of domain classifier\n",
    "            _, domain_preds = torch.max(domain_preds, 1)\n",
    "            target_domain_correct += (domain_preds == domain_labels).sum().item()\n",
    "            predicted_labels += label_preds\n",
    "            true_labels += labels\n",
    "            predicted_domain += domain_preds\n",
    "        conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "        print('Credit Confusion Matrix Black: ')\n",
    "        print(conf_matrix)\n",
    "        fpr = conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0]) \n",
    "        print('False positive black: ', fpr)\n",
    "        \n",
    "        # predictive Parity\n",
    "        metric_frame = MetricFrame(metrics=accuracy_score, y_true=true_labels_white + true_labels,\n",
    "                           y_pred=predicted_labels_white + predicted_labels,\n",
    "                           sensitive_features=[1]*len(true_labels_white) + [0]*len(true_labels)) #are white and black mixed up here?\n",
    "\n",
    "        predictive_parity = metric_frame.difference(method='between_groups')\n",
    "\n",
    "        print(\"Predictive parity: \", predictive_parity)\n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df988afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lc(model, lambda_):\n",
    "    model.eval()\n",
    "    model.lc.eval()\n",
    "    model.f.eval()\n",
    "    correct = 0\n",
    "    ones = 0\n",
    "    for data, labels in source_test:\n",
    "        embedding = model.f(data)\n",
    "        label_preds = model.lc(embedding)\n",
    "        print(label_preds)\n",
    "        label_preds = torch.max(label_preds, 1)\n",
    "        print(label_preds)\n",
    "        # Get number of correct predictions\n",
    "        source_label_correct += (label_preds == labels).sum().item()\n",
    "        ones += list(label_preds.numpy()).count(1)\n",
    "        \n",
    "        # get number of ones\n",
    "    source_test_label_acc = source_label_correct / len(source_test_loader.dataset)\n",
    "    print('Accuracy of label classifier on source test: ', source_test_label_acc )\n",
    "    print('Late loans predicted on source test: ', ones/ len(source_test_loader.dataset))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c741c466",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dc(model, lambda_):\n",
    "    model.eval()\n",
    "    model.lc.eval()\n",
    "    model.f.eval()\n",
    "    model.dc.eval()\n",
    "    correct = 0\n",
    "    black = 0\n",
    "    actually_black = 0\n",
    "    for data, labels in combined_loader:\n",
    "        combined_data, domain_labels = data, labels\n",
    "        embedded = grad_reverse(model.f(combined_data), lambda_=lambda_)\n",
    "        domain_preds = model.dc(embedded)\n",
    "        #print(domain_preds)\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        #domain_loss = domain_criterion(domain_preds, domain_labels)\n",
    "        #print(domain_preds)\n",
    "        correct += (domain_preds == labels).sum().item()\n",
    "        black += list(domain_preds.numpy()).count(1)\n",
    "        actually_black += list(labels.numpy()).count(1)\n",
    "    accuracy = correct / len(combined_loader.dataset)\n",
    "    print('domain accuracy on train data: ', accuracy)\n",
    "    print('black predicted: ', black/(len(combined_loader.dataset)))\n",
    "    print('num actually black: ', actually_black/(len(combined_loader.dataset)))\n",
    "    black = 0\n",
    "    correct = 0\n",
    "    actually_black = 0\n",
    "    for data, labels in source_test:\n",
    "        embedded = grad_reverse(model.f(data), lambda_=lambda_)\n",
    "        domain_preds = model.dc(embedded)\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        labels = torch.zeros(data.shape[0]).long()\n",
    "        correct += (domain_preds == labels).sum().item()\n",
    "        black += list(domain_preds.numpy()).count(1)\n",
    "    for data, labels in target_test:\n",
    "        embedded = grad_reverse(model.f(data), lambda_=lambda_)\n",
    "        domain_preds = model.dc(embedded)\n",
    "        _, domain_preds = torch.max(domain_preds, 1)\n",
    "        labels = torch.ones(data.shape[0]).long()\n",
    "        correct += (domain_preds == labels).sum().item()\n",
    "        black += list(domain_preds.numpy()).count(1)\n",
    "    test_accuracy = correct / (len(target_test.dataset)+ len(source_test.dataset))\n",
    "    print('domain test accuracy: ', test_accuracy)\n",
    "    print('black pred test: ', black/(len(target_test.dataset)+ len(source_test.dataset)))\n",
    "    print('actual black = ', len(target_test.dataset)/(len(target_test.dataset)+ len(source_test.dataset)))\n",
    "          \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf4b249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dann, num_epochs, feature_optimizer, label_epochs, domain_epochs, source_loader, target_loader, label_criterion, domain_criterion,\n",
    "         label_optimizer, domain_optimizer, lambda_):\n",
    "    for epoch in range(num_epochs):\n",
    "    # Set the DANN model to training mode\n",
    "        dann.train()\n",
    "        # Set the number of batches for each epoch\n",
    "       \n",
    "\n",
    "            \n",
    "        # Train the label classifier for label_epochs epochs\n",
    "        for j in range(label_epochs):\n",
    "            dann.lc.train()\n",
    "            for data, labels in combined_label_loader:\n",
    "                source_data, source_labels = data, labels\n",
    "                label_optimizer.zero_grad()\n",
    "                feature_embed = dann.f(source_data)\n",
    "                label_preds = dann.lc(feature_embed)\n",
    "                label_loss = label_criterion(label_preds, source_labels)\n",
    "                label_loss.backward()\n",
    "                label_optimizer.step()\n",
    "        print('label trained')\n",
    "        dann.lc.eval()\n",
    "\n",
    "        # Train the domain classifier for domain_epochs epochs\n",
    "        for j in range(domain_epochs):\n",
    "            dann.dc.train()\n",
    "            for data, labels in combined_loader:\n",
    "                combined_data, domain_labels = data, labels\n",
    "                domain_optimizer.zero_grad()\n",
    "                embedded = grad_reverse(dann.f(combined_data), lambda_=lambda_)\n",
    "                domain_preds = dann.dc(embedded)\n",
    "                domain_loss = domain_criterion(domain_preds, domain_labels)\n",
    "                domain_loss.backward()\n",
    "                domain_optimizer.step()\n",
    "        print('domain trained')\n",
    "        dann.dc.eval()\n",
    "\n",
    "\n",
    "        dann.f.train()\n",
    "        dann.dc.train()\n",
    "        dann.lc.train()\n",
    "        dann.train()\n",
    "        for data, labels in source_loader:\n",
    "            feature_optimizer.zero_grad()\n",
    "            #dann.zero_grad()\n",
    "            #feature_embed_label, feature_embed_domain = dann(data, lambda_)\n",
    "            #feature_embed_domain = grad_reverse(dann.f(data), lambda_=lambda_)\n",
    "            #label_preds = dann.lc(feature_embed_label)\n",
    "            #domain_preds = dann.dc(feature_embed_domain)\n",
    "            label_preds, domain_preds = dann(data, lambda_)\n",
    "            label_loss = label_criterion(label_preds, labels)\n",
    "            domain_labels = torch.zeros(len(data), dtype=torch.long)\n",
    "            domain_loss = domain_criterion(domain_preds, domain_labels)\n",
    "            total_loss = label_loss + (domain_loss* lambda_)\n",
    "            total_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "            \n",
    "        for data, labels in target_loader:\n",
    "            feature_optimizer.zero_grad()\n",
    "            #dann.zero_grad()\n",
    "            #feature_embed = dann.f(data)\n",
    "            #label_preds = dann.lc(feature_embed)\n",
    "            #feature_embed_domain = grad_reverse(dann.f(data), lambda_=lambda_)\n",
    "            #domain_preds=dann.dc(feature_embed_domain)\n",
    "            #print(domain_preds)\n",
    "            label_preds, domain_preds = dann(data, lambda_)\n",
    "            label_loss = label_criterion(label_preds, labels)\n",
    "            domain_labels = torch.ones(len(data), dtype=torch.long)\n",
    "            domain_loss = domain_criterion(domain_preds, domain_labels)\n",
    "            total_loss = label_loss + (domain_loss*lambda_)\n",
    "            total_loss.backward()\n",
    "            feature_optimizer.step()\n",
    "        print('feature extractor trained')\n",
    "        dann.f.eval()\n",
    "        dann.dc.eval()\n",
    "        dann.lc.eval()\n",
    "        dann.eval()\n",
    "        print('Epoch [{}/{}], Label Loss: {:.4f}, Domain Loss: {:.4f}'.format(epoch+1, num_epochs, label_loss.item(), domain_loss.item()))\n",
    "        test(dann, source_train, source_test, target_test, lambda_)\n",
    "        #test_dc(dann, lambda_)\n",
    "        #test_fairness(dann, lambda_)\n",
    "        print()\n",
    "\n",
    "# Print the loss for this epoch\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad66b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_lambda(lambda_):\n",
    "    model = Dann()\n",
    "\n",
    "    # initialize weights\n",
    "    for m in model.f.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)\n",
    "    for m in model.lc.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)\n",
    "    for m in model.dc.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            init.xavier_uniform_(m.weight)\n",
    "\n",
    "    # Define optimizer for the feature extractor and label classifier\n",
    "    optimizer_lc = torch.optim.Adam(list(model.f.parameters()) + list(model.lc.parameters()), lr=0.0001, weight_decay=1e-3)\n",
    "    criterion_lc = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define optimizer for the domain classifier\n",
    "    optimizer_dc = torch.optim.Adam(model.dc.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "    criterion_dc = nn.CrossEntropyLoss()\n",
    "\n",
    "    f_optimizer = torch.optim.Adam(model.f.parameters(), lr=0.0001, weight_decay=1e-3)\n",
    "    # Define number of epochs to train the label and domain classifiers\n",
    "    n_lc_epochs = 10\n",
    "    n_dc_epochs = 10\n",
    "\n",
    "    _lambda = lambda_\n",
    "    lambda_ = _lambda\n",
    "    # Training loop\n",
    "    num_epochs = 32\n",
    "    train(model, num_epochs, optimizer_lc, 20, domain_epochs =30, \n",
    "          source_loader=source_train, target_loader= target_train, \n",
    "          label_criterion =criterion_lc, domain_criterion=criterion_dc,\n",
    "         label_optimizer = optimizer_lc, domain_optimizer = optimizer_dc, lambda_ = lambda_)\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d123c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(lam, model, test_src, test_targ):\n",
    "    file_name = 'results_mult_'+str(lam)+'.txt'\n",
    "    my_str = 'Lambda: ' +str(lam) + '\\n\\n'\n",
    "    results = get_results(model, test_src, test_targ, lam)\n",
    "    # test accuracy-- source, target, and overall\n",
    "    my_str += 'Label Classifier Accuracy (Test Data) \\n'\n",
    "    my_str += 'Source (White): ' + str(results['source_label_accuracy']) + '\\n'\n",
    "    my_str += 'Target (Black): ' + str(results['target_label_accuracy'])+ '\\n'\n",
    "    my_str += 'Overall: ' + str(results['overall_label_accuracy'])+ '\\n\\n'\n",
    "    # domain test accuracy-- source, target, and overall\n",
    "    my_str += 'Domain Classifier Accuracy (Test Data) \\n'\n",
    "    my_str += 'Source (White): ' + str(results['source_domain_accuracy']) + '\\n'\n",
    "    my_str += 'Target (Black): ' + str(results['target_domain_accuracy'])+ '\\n'\n",
    "    my_str += 'Overall: ' + str(results['overall_domain_accuracy'])+ '\\n\\n'\n",
    "    # false positives-- source, target, and overall\n",
    "    my_str += 'False Positive Rate (Test Data) \\n'\n",
    "    my_str += 'Source (White): ' + str(results['fpr_white']) + '\\n'\n",
    "    my_str += 'Target (Black): ' + str(results['fpr_black'])+ '\\n'\n",
    "    my_str += '(Black FPR) / (White FPR): ' +  str(results['fpr_black']/results['fpr_white'])+ '\\n'\n",
    "    my_str += 'Overall: ' + str(results['fpr_overall'])+ '\\n\\n'\n",
    "    \n",
    "    print(my_str)\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(my_str)\n",
    "    \n",
    "    # parity\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3372bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(model, test_src, test_targ, lam):\n",
    "    #print('get results model')\n",
    "    #print(model)\n",
    "    with torch.no_grad():\n",
    "        # Label predictor metrics\n",
    "        source_correct = 0\n",
    "        source_label_predictions = []\n",
    "        source_label_true = []\n",
    "        \n",
    "        # Domain predictor metrics\n",
    "        source_dom_correct = 0\n",
    "        source_dom_predictions = []\n",
    "        source_dom_true = []\n",
    "        for data, labels in test_src:\n",
    "            # Make label predictions\n",
    "            embedded = model.f(data)\n",
    "            label_preds = model.lc(embedded)\n",
    "            _, label_preds = torch.max(label_preds, 1)\n",
    "            source_correct += (label_preds == labels).sum().item()\n",
    "            source_label_predictions += label_preds\n",
    "            source_label_true+= labels\n",
    "            \n",
    "            # Make domain predictions\n",
    "            domain_embedded = grad_reverse(model.f(data), lam)\n",
    "            domain_preds = model.dc(domain_embedded)\n",
    "            _, domain_preds = torch.max(domain_preds, 1)\n",
    "            domain_true = torch.zeros(data.shape[0]).long()\n",
    "            source_dom_correct += (domain_preds == domain_true).sum().item()\n",
    "            source_dom_predictions += domain_preds\n",
    "            source_dom_true += domain_true\n",
    "        source_label_accuracy = 100.* source_correct / len(test_src.dataset)\n",
    "        source_domain_accuracy = 100 * source_dom_correct / len(test_src.dataset)\n",
    "        source_label_positive_rate = (sum(source_label_predictions)/len(source_label_predictions)).item()\n",
    "        \n",
    "        \n",
    "        \n",
    "        # FOR TARGET DATA\n",
    "        # Label predictor metrics\n",
    "        target_correct = 0\n",
    "        target_label_predictions = []\n",
    "        target_label_true = []\n",
    "        \n",
    "        # Domain predictor metrics\n",
    "        target_dom_correct = 0\n",
    "        target_dom_predictions = []\n",
    "        target_dom_true = []\n",
    "        for data, labels in test_targ:\n",
    "            # Make label predictions\n",
    "            embedded = model.f(data)\n",
    "            label_preds = model.lc(embedded)\n",
    "            _, label_preds = torch.max(label_preds, 1)\n",
    "            target_correct += (label_preds == labels).sum().item()\n",
    "            target_label_predictions += label_preds\n",
    "            target_label_true+= labels\n",
    "            \n",
    "            # Make domain predictions\n",
    "            domain_embedded = grad_reverse(model.f(data), lam)\n",
    "            domain_preds = model.dc(domain_embedded)\n",
    "            _, domain_preds = torch.max(domain_preds, 1)\n",
    "            domain_true = torch.ones(data.shape[0]).long()\n",
    "            target_dom_correct += (domain_preds == domain_true).sum().item()\n",
    "            target_dom_predictions += domain_preds\n",
    "            target_dom_true += domain_true\n",
    "        target_label_accuracy = 100.* target_correct / len(test_targ.dataset)\n",
    "        target_domain_accuracy = 100 * target_dom_correct / len(test_targ.dataset)\n",
    "        target_label_positive_rate = (sum(target_label_predictions)/len(target_label_predictions)).item()\n",
    "        \n",
    "        # OVERALL\n",
    "        overall_label_accuracy = 100.*(target_correct + source_correct)/(len(test_src.dataset)+len(test_targ.dataset))\n",
    "        overall_domain_accuracy = 100.*(target_dom_correct + source_dom_correct)/(len(test_src.dataset)+len(test_targ.dataset))\n",
    "        overall_label_positive_rate = ((sum(source_label_predictions) + sum(target_label_predictions))/(len(target_label_predictions)+len(source_label_predictions))).item()\n",
    "        \n",
    "        # False positive rates\n",
    "        conf_matrix = confusion_matrix(source_label_true, source_label_predictions)\n",
    "        fpr_white = 100.* conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])\n",
    "        \n",
    "        conf_matrix = confusion_matrix(target_label_true, target_label_predictions)\n",
    "        fpr_black = 100.* conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])\n",
    "        \n",
    "        conf_matrix = confusion_matrix(target_label_true+source_label_true, target_label_predictions+source_label_predictions)\n",
    "        fpr_overall = 100. *conf_matrix[0, 1] / (conf_matrix[0, 1] + conf_matrix[0, 0])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        results = {'source_label_accuracy': source_label_accuracy, 'source_domain_accuracy':source_domain_accuracy,\n",
    "                  'target_label_accuracy': target_label_accuracy, 'target_domain_accuracy':target_domain_accuracy,\n",
    "                   'overall_label_accuracy': overall_label_accuracy, 'overall_domain_accuracy': overall_domain_accuracy,\n",
    "                   'fpr_white': fpr_white, 'fpr_black': fpr_black, 'fpr_overall': fpr_overall,\n",
    "                   'source_label_positive_rate': source_label_positive_rate, 'target_label_positive_rate': target_label_positive_rate,\n",
    "                   'overall_label_positive_rate': overall_label_positive_rate}\n",
    "        return results\n",
    "        \n",
    "        \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ddfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for lam in range(10):\n",
    "    lam = 0.1*lam\n",
    "    model = train_for_lambda(lam)\n",
    "    models[lam] = model\n",
    "    write_results(lam, model, source_test, target_test)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf351c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_conf_results(lam, test_src, test_targ):\n",
    "    file_name = 'results_conf_SCF_Race_'+str(lam)+'.txt'\n",
    "    my_str = 'Lambda: ' +str(lam) + '\\n\\n'\n",
    "    for i in range(10):\n",
    "        my_str += 'run: ' + str(i+1)+'/15\\n'\n",
    "        model = train_for_lambda(lam)\n",
    "        # write model to file\n",
    "        model_file_name = 'model_results/model_SCF_Race'+str(lam)+'_'+str(i)+'.pt'\n",
    "        # save in foler model_results\n",
    "        path = 'model_results/' + file_name\n",
    "        torch.save(model, path)\n",
    "        results = get_results(model, test_src, test_targ, lam)\n",
    "        # test accuracy-- source, target, and overall\n",
    "        my_str += 'Label Classifier Accuracy (Test Data) \\n'\n",
    "        my_str += 'Source (White): ' + str(results['source_label_accuracy']) + '\\n'\n",
    "        my_str += 'Target (Black): ' + str(results['target_label_accuracy'])+ '\\n'\n",
    "        my_str += 'Overall: ' + str(results['overall_label_accuracy'])+ '\\n\\n'\n",
    "        # domain test accuracy-- source, target, and overall\n",
    "        my_str += 'Domain Classifier Accuracy (Test Data) \\n'\n",
    "        my_str += 'Source (White): ' + str(results['source_domain_accuracy']) + '\\n'\n",
    "        my_str += 'Target (Black): ' + str(results['target_domain_accuracy'])+ '\\n'\n",
    "        my_str += 'Overall: ' + str(results['overall_domain_accuracy'])+ '\\n\\n'\n",
    "        # false positives-- source, target, and overall\n",
    "        my_str += 'False Positive Rate (Test Data) \\n'\n",
    "        my_str += 'Source (White): ' + str(results['fpr_white']) + '\\n'\n",
    "        my_str += 'Target (Black): ' + str(results['fpr_black'])+ '\\n'\n",
    "        my_str += '(Black FPR) / (White FPR): ' +  str(results['fpr_black']/results['fpr_white'])+ '\\n'\n",
    "        my_str += 'Overall: ' + str(results['fpr_overall'])+ '\\n\\n'\n",
    "        # parity\n",
    "        my_str += 'Source Label Positive Rate: ' + str(results['source_label_positive_rate']) + '\\n'\n",
    "        my_str += 'Target Label Positive Rate: ' + str(results['target_label_positive_rate']) + '\\n'\n",
    "        my_str += 'Overall Label Positive Rate: ' + str(results['overall_label_positive_rate']) + '\\n\\n'\n",
    "\n",
    "        print(my_str)\n",
    "\n",
    "    print(my_str)\n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(my_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6955aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To get parity, changed get_results() and write_conf_results() to include positive rate\n",
    "for l in range(10):\n",
    "    l = l * .1\n",
    "    write_conf_results(l, source_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b71c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dec4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get true positive and negative rates for test data !!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ae9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_pos_black = sum(y_test_target)\n",
    "print('1s in y_test_target', true_pos_black)\n",
    "true_pos_white = sum(y_test_source)\n",
    "print('1s in y_test_source', true_pos_white)\n",
    "\n",
    "length_white = len(y_test_source)\n",
    "length_black = len(y_test_target)\n",
    "\n",
    "true_neg_black = len(y_test_target) - true_pos_black\n",
    "print('zeros in test black ', true_neg_black)\n",
    "true_neg_white = len(y_test_source) - true_pos_white\n",
    "print('zeros in test white ', true_neg_white)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a1efab",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 1\n",
    "write_conf_results(l, source_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa8e994",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 2\n",
    "model = train_for_lambda(l)\n",
    "write_results(l, model, source_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae3ce4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 3\n",
    "model = train_for_lambda(l)\n",
    "write_results(l, model, source_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55bd3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = 15\n",
    "model = train_for_lambda(l)\n",
    "write_results(l, model, source_test, target_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
